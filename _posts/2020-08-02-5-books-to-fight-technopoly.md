---
title: 5 Books to Fight Technopoly
published: true
description: Humans do not exist to serve technology or Technopolists.
tags: reading, technology, books, history
cover_image: https://thepracticaldev.s3.amazonaws.com/i/8xtahvwsl1647s9x67nw.jpg
---

This era is a time of unprecedented technological advance. Every day, someone jumps out of the woodwork with a new gadget that they claim will solve a problem you don't even know you have. Perhaps it does. However, there's an implicit demand from all of this technology and automation that usually goes unexamined by the zeitgeist: ignore the unintended consequences. Ignore the intrusion of advertising in public places previously immune to it. Ignore the predatory surveillance. Ignore [the actual genocide facilitated by our platform](https://www.nytimes.com/2018/10/15/technology/myanmar-facebook-genocide.html).

We've been dealing with unintentional consequences of technology since the invention of the alphabet (and it *was* an invented technology) harmed our ability to remember things. We've been able to adapt to such changes over long periods of time and make trade-offs that benefited us, but today's technologists are not interested in the time it takes for technology to assimilate. As such, their stated good intentions fall flat next to their destructive effects.

We too often default to viewing new technology as good in and of itself, as the Technopolists wish. Then we fail to follow that up with the serious discussion of the effects of that technology that these changes demand. When those downsides become so apparent that we can no longer ignore them, we treat that introduced technology as though it had always existed, making opposition to it unthinkable. If we're allowed to pursue reforms at all, proposed limitations are curtailed because our culture still thinks of technology as a good end in and of itself. The elimination of actual harms does not seem to be required. And then, if we get minimal protections for people, the limitations are perpetually at risk of being destroyed by government corruption like Net Neutrality was.

I don't know if it makes the situation better or worse, but at each reckless step, there have been detractors. They do not usually object to the concept of modern technology itself. Rather, they object to the mindset that technology and change are inherently good, and that technology's indiscriminate advance is strictly good for society. In challenging that orthodoxy, these writers show us inconvenient truths that exploitative technologists are happy to hide. Here is a list of 5 such books that are worth reading.

# 1) [*Technopoly*](https://en.wikipedia.org/wiki/Technopoly) (1992) by Neil Postman
![Technopoly cover](https://thepracticaldev.s3.amazonaws.com/i/glfzt9j3h7vu6c0oio3f.jpg)

*Technopoly* is the most important book about technology of the 20th Century.

The short book is dense with information and frameworks for how to think about abuse of technology. Frequently, we casually allow our tools, whether machine or ideology, to shape culture in ways that benefit the few at the great expense of the many's very humanity. 

*Technopoly* is social criticism about technology, but it is not an anti-technology book; it is an anti-Technopoly book. Technopoly is a collection of radical normative claims about how technology should shape the world that people have increasingly adopted since invention became tied to commercialism and efficiency. It may seem obvious to us that more efficiency is inherently good, as this mindset pervades our culture, but this belies a gross disregard for other people. The Technopolist encourages others to denigrate the past in order to push their new products, disregarding alternative opinions as "Luddite." It places that efficiency of operation above consideration of the abilities of human beings, leading to the creation of new systems that don't serve humans well and the increasing estrangement of us from functional older systems in ways that make our lives strictly worse.

This happens because the Technopolist does not care about leveraging technology intelligently to pursue a better world. Instead, they pursue new technology as the end in and of itself, claiming that will be the cure to human suffering because because human suffering is caused by human inefficiencies. This leads to creating things without consideration of any negative consequences that reveal themselves later. Basically, they say "move fast and break things," because they very wrongly believe that the things that break couldn't matter. It is morbid misanthropy couched in techno-utopianism.

Further, as much as they phrase these ideals as the cure to society's ills for PR purposes, the reality is that our most difficult problems are social and cannot adequately be solved by the addition of new technology. Indiscriminate applications of technology can make them exponentially worse. One of the key reasons for this is that, technology does not sit neutrally in our world. The fact that technology can radically change the way that someone approaches one problem at hand is only the most germane possible consequence. A change to technological conditions can also radically change a person's sense of self. For example, consider how the addition of a vehicle fundamentally changes someone's sense of how big the world is, or how people in skilled jobs that are increasingly automated begin to view themselves as unskilled monitors. On top of that, it's important to realize that technology is *designed*, and we must be vigilant in understanding that the technologists behind it may want to direct that change to benefit themselves above others, such as how Facebook made itself somewhat indispensable by [fundamentally changing the way interpersonal networking functioned with its emotional labor machine](https://www.theverge.com/2018/4/28/17293056/facebook-deletefacebook-social-network-monopoly). 

*Technopoly* reminds us that technology exists to serve us; humans do not exist to serve technology. It is important to actually analyze and evaluate the effects of new technologies instead of blindly moving from one to the next and assuming the answer *must* be a *new* technology. Furthermore, we must keep our guards up against Technopolists' encouragement to abandon our cultural memory. Often you'll find that many new applications of technology that have caused massive social welfare problems in our time are shockingly new. 

* Every company's leveraging of the internet to insert itself into your life or curtail your expectations of privacy must have come after the Internet, which only became commercially available about 30 years ago.
* Andrew Wakefield's fraudulent anti-vaccination paper, which inspired much of the current wave of anti-vaccination insanity, was published in 1998.
* Movie theatres once treated your ticket as the only payment necessary to see the film, and they only began requiring you to watch pre-film commercials in the mid-1990s.
* The Originalist approach to the United States Constitution, used to provide inappropriate second-hand legitimacy to extreme conservative politics in the judiciary, was [essentially invented and formalized in the 1970s](https://www.commondreams.org/views/2018/09/04/brett-kavanaugh-and-triumph-conservative-counterrevolution), boosted by the [Powell Manifesto](http://reclaimdemocracy.org/powell_memo_lewis/).

These are blips in the cultural consciousness, but their creators want you to believe they are immutable and inextricable features of our world, that have always been this way and always shall be. They did not, and constant reevaluation of their place is essential so that their real consequences may be understood and properly mitigated. Technology, whether tool or toy or ideology, must be treated seriously and critically, not as a sacred cow.

# 2) [*Geek Heresy*](https://geekheresy.org/) (2015) by Kentaro Toyama
![Geek Heresy cover](https://thepracticaldev.s3.amazonaws.com/i/62rx87aw637so6c94ven.jpg)
Also a contender for a list of the worst-titled books, *Geek Heresy* is not really about being a geek, nor necessarily a heretic. The title is sort-of a preemptive and misguided apology to its target audience, technologists who believe that any social problem can be adequately addressed by installing some targeted software and hardware solution. This is completely untrue, and Toyama's explanation is a simple aphorism, *The Law of Amplification*: "Like a lever, technology amplifies people's capacities in the direction of their intentions." To demonstrate the implications of this Law, Toyama walks us through his considerable experiences as a technological innovator trying to modernize rural populations in India.

It is extremely common for paternalistic groups to target people with fewer technological resources, donate a number of computers with an educational program, then pat themselves on the back while leaving those people to fend for themselves. Often, they don't succeed, because of the Law of Amplification: the technology amplifies what the people want to do with it. If someone is intrinsically interested in educating themselves, they are more likely to use a computer to explore new things and do that exponentially more effectively than they would without one. If they are intrinsically interested in entertaining themselves, as most children tend to be, then the addition of computers can be a distraction in class exponentially worse than anything else. As a result, many (most?) of these attempts at indiscriminately throwing technology at problems results in little to no betterment of these peoples' lives and they reject them entirely. The book is peppered with examples of what happens to the donated computers when they are taken out of classrooms as ineffective. And where are the benefactors while these programs fail? Shopping around the results of earlier, smaller, successful pilots for the failed program to new areas, never looking back.

The Law of Amplification is more general than education. It's useful for evaluating the purported goals of any technology, and helpful for spotting the unintended consequences that are the core of Toyama's book. For example, Facebook's technology *amplifies* its *capacity* to surveil and sell advertising in the direction of its *intention* to reach the world population. [*Adcreep* by Mark Bartholemew](https://www.sup.org/books/title/?id=25991) is a good book to read if you're curious about the strange, unpleasant, and unprecedented places our unregulated advertising regime is taking us.

# 3) [*Race After Technology*](https://www.ruhabenjamin.com/race-after-technology) (2019) by Ruha Benjamin
![Race After Technology cover](https://dev-to-uploads.s3.amazonaws.com/i/vy8wwxlpfgkfc1vkfjdy.jpg)

*Race After Technology* is essential. It is a point-by-point survey of the ways that our current approach to the development of software creates and enforces artificial differences between groups of people. Our world has been constructed with design decisions that make thriving in everyday life easy for some and impossible for many others. Obviously, the book has its eye turned toward race, but the same is equally true for other groups of people who are not seen as the "default" in a society. The book looks at the issue through the lens of the "New Jim Code":

> The employment of new technologies that reflect and reproduce existing inequities but that are promoted and perceived as more objective or progressive than the discriminating systems of a previous era.

Technology has a certain amount of power to shape the way that we think. The idea of using it to correct the sins of our world is very appealing. However, the sins of our world are social problems and technology is as sensitive to them as it is designed to be. Not only that, once a technological system is in place and automating decisions, it can be a nightmare to replace because our cognition of the system changes when we think of its results as computerized and calculated and correct.

In other words, technology is a tool and the way those tools are implemented matters. Malicious technologies amplify "traditional" inequities in very predictable ways, and reading this book will show you how to spot them in the wild. Even better, reading it will give you insights into the social dynamics behind those technological decisions, because all of those approaches to social problems were designed in a particular social context. For example, the "datafication of injustice" is everywhere, even before it is used to justify encoding biases into machines—think of how "black-on-black crime" statistics are used to justify increased policing of black people when the same is never true of "white-on-white crime." Well, those same statistical tricks are used to justify algorithms that automate the decision to police black neighborhoods more.

What the New Jim Code represents is a failure to listen to the voices of the people we know are oppressed. People who do not want to engage in that necessary work, who are ignorant of these issues or else revel in their complicity with them, have been systematically put in power for decades. If you are not those people, you have to work under those people, and those people have spent a huge amount of time and money in making analyses of systemic racism seem ridiculous. You owe it to yourself to explicitly correct subtly white supremacist ideas that have permeated your mind.

# 4) [*The Glass Cage*](http://www.nicholascarr.com/?page_id=18) (2014) by Nicholas Carr
![The Glass Cage cover](https://thepracticaldev.s3.amazonaws.com/i/mu64pj4z5pmu2k77l9yh.jpg)

This book is Carr's exploration of the potential effects of automation in modern life. Of course, most people like automation in the sense that they can pick up their phones and look up information or they love the idea of a world with cars that drive themselves. However, Carr and the many researchers whose work he distills in this book have very credible worries that the insatiable breadth and expansion of modern automation is currently having negative effects. We expand automation to new fields under the banner of alleviating human error and making people not have to work as hard, but it often does the opposite: automating systems that require skill can cause the people to become complacent.

That's not because these people are lazy or unnecessary, as a technologist might say. It's because they are human and these systems are being designed in ways that don't work with humans. They fundamentally change the nature of the work, and indeed the nature of the people who do that work, so they go from craftsmen to glorified monitors. These jobs go from interesting work that a person may become good at and find satisfying to work that is, in many ways, impossible for humans to do. In fact, the problem is often compounded further when these systems are designed to be openly hostile and distrusting of humans. Developers of these hardware and software solutions come to think of these users as idiots whose mistakes must be abstracted away in the nature of efficiency, even though they are designing systems that quantifiably drive people to make more mistakes than non-automated systems.

# 5) [*The Design of Everyday Things*](https://mitpress.mit.edu/books/design-everyday-things) (1988) by Donald Norman
![The Design of Everyday Things cover](https://thepracticaldev.s3.amazonaws.com/i/hcspkwy6nr4zm93gas74.jpg)

Most of this post has been doom-and-gloom because of how deeply this Technopoly thinking has permeated modern day culture. As these books show, it has been opposed intellectually, but we don't see much material resistance to it otherwise. It does exist in the fields of human factors and ergonomics, of which this is an approachable and seminal book.

*The Design of Everyday Things* is about a lot of specific ways that we build things that don't seem to be made for a world inhabited by humans, for use by humans. The most famous examples used in this book are, of course, the [Norman Doors](https://99percentinvisible.org/article/norman-doors-dont-know-whether-push-pull-blame-design/), tools that we have been creating for hundreds of years and yet still make new models of that require signs and labels to tell us how to open them...because, without those, everyone opens the door incorrectly. They do not match our mental models of the handle and door, and this is *not* your fault. One of the things that designers get to take advantage of is people's instinctive and habitual predilection for apologizing to machines. The machines get the benefit of the doubt because we assume we must have been wrong and need to become more skilled. Norman's radical take on this dynamic was to place the blame where it belongs: with the designer. We don't open the door incorrectly. The door was designed incorrectly, so that it was at odds with the mental models of the people who would use it.

These examples may often be funny, but they are not necessarily accidents because they are often the product of a profound lack of empathy on the part of the designer.

Also, apparently, Donald Norman also wrote a book called [*Things That Make Us Smart: Defending Human Attributes In The Age Of The Machine*](https://www.jnd.org/books/things-that-make-us-smart-defending-human-attributes-in-the-age-of-the-machine.html), which has a very appealing title. In [*Living with Complexity*](https://www.jnd.org/books/living-with-complexity.html), which I have read, Norman's explicit demarcation of complexity and confusion as states of the world and mind is extremely useful and Good.

# Future Reading
I'm self-conscious about most of this list being written by white men, so here are some books I'm planning to read that aren't:
* [*A People’s History of Computing in the United States*](http://www.hup.harvard.edu/catalog.php?isbn=9780674970977) (2018) by Joy Lisi Rankin
* [*Programmed Inequality: How Britain Discarded Women Technologists and Lost Its Edge in Computing*](https://mitpress.mit.edu/books/programmed-inequality) (2017) by Marie Hicks
* [*Dark Matters: On the Surveillance of Blackness*](https://www.dukeupress.edu/dark-matters) (2015) by Simone Browne
* [*Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy*](https://weaponsofmathdestructionbook.com/) (2016) by Cathy O'Neil
